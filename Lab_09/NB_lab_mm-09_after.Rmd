---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---

## Математическое моделирование

### Практика 9

### Машины опорных векторов        

В практических примерах ниже показано как:   

* обучить модель классификатора на опорных векторах     
* обучить модель SVM с различными формами ядерной функции     
* строить ROC-кривые встроенными функциями R   

*Модели*: SVM   
*Данные*: `Khan {ISLR}`   

Подробные комментарии к коду лабораторных см. в [1], глава 9.   

```{r, message = F, warning = F}
library('e1071')     # SVM
library('ROCR')      # ROC-кривые
library('ISLR')      # данные по экспрессии генов

my.seed <- 1

```

## Классификатор на опорных векторах    

### Cгенерированные данные: два линейно неразделимых класса

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}

# создаём наблюдения
set.seed(my.seed)
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1

# данные не разделяются линейно
plot(x, pch = 19, col = (3 - y)) 

# таблица с данными, отклик -- фактор
dat <- data.frame(x = x, y = as.factor(y))

# классификатор на опорных векторах с линейной границей
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)

# на графике опорные наблюдения показаны крестиками
plot(svmfit, dat)

# список опорных векторов
svmfit$index

# сводка по модели
summary(svmfit)

# уменьшаем штрафной параметр
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit, dat)
svmfit$index

# делаем перекрёстную проверку, изменяя штраф (аргумент cost)
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
# лучшая модель -- с минимальной ошибкой
bestmod <- tune.out$best.model
summary(bestmod)

# генерируем контрольные данные
xtest <- matrix(rnorm(20*2), ncol = 2)
ytest <- sample(c(-1,1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))

# делаем прогноз по лучшей модели
ypred <- predict(bestmod, testdat)

# матрица неточностей
table(predict = ypred, truth = testdat$y)

# прогноз по модели с cost = 0.01
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = .01, scale = FALSE)
ypred <- predict(svmfit, testdat)
# матрица неточностей
table(predict = ypred, truth = testdat$y)
```

### Сгенерированные данные: два линейно разделимых класса    

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}
# создаём наблюдения
x[y == 1, ] <- x[y == 1, ] + 0.5
plot(x, col = (y+5)/2, pch = 19)

# таблица с данными, отклик -- фактор
dat <- data.frame(x = x, y = as.factor(y))

# очень большой cost (маленький зазор, высокая точность классификации)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit, dat)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit,dat)
```


## Машина опорных векторов    

### Сгенерированные данные: нелинейная граница между классами   

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}
# создаём наблюдения
set.seed(1)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))

# таблица с данными, отклик -- фактор 
dat <- data.frame(x = x, y = as.factor(y))
plot(x, col = y, pch = 19)

# обучающая выборка
train <- sample(200, 100)

# SVM с радиальным ядром и маленьким cost
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
summary(svmfit)

# SVM с радиальным ядром и большим cost
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])

# перекрёстная проверка
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train, ], kernel = "radial", 
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
# матрица неточностей для прогноза по лучшей модели
table(true = dat[-train, "y"], 
      pred = predict(tune.out$best.model, newdata = dat[-train, ]))
```

## ROC-кривые

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}

# функция построения ROC-кривой: pred -- прогноз, truth -- факт
rocplot <- function(pred, truth, ...){
    predob = prediction(pred, truth)
    perf = performance(predob, "tpr", "fpr")
    plot(perf,...)}

# последняя оптимальная модель
svmfit.opt <- svm(y ~ ., data = dat[train, ], 
                  kernel = "radial", gamma = 2, cost = 1, decision.values = T)

# количественные модельные значения, на основе которых присваивается класс
fitted <- attributes(predict(svmfit.opt, dat[train, ],
                             decision.values = TRUE))$decision.values

# график для обучающей выборки
par(mfrow = c(1, 2))
rocplot(fitted, dat[train, "y"], main = "Training Data")

# более гибкая модель (gamma выше)
svmfit.flex = svm(y ~ ., data = dat[train, ], kernel = "radial", 
                  gamma = 50, cost = 1, decision.values = T)
fitted <- attributes(predict(svmfit.flex, dat[train, ], 
                             decision.values = T))$decision.values
rocplot(fitted, dat[train,"y"], add = T, col = "red")

# график для тестовой выборки
fitted <- attributes(predict(svmfit.opt, dat[-train, ], 
                             decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], main = "Test Data")
fitted <- attributes(predict(svmfit.flex, dat[-train, ], 
                             decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], add = T, col = "red")
```

## SVM с несколькими классами    

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}

# генерируем данные
set.seed(1)
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0, 50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))

# график и модель по методу "один против одного"
par(mfrow = c(1, 1))
plot(x, col = (y + 1))
svmfit = svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
plot(svmfit, dat)
```

## Анализ данных по уровню экспрессии генов   

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}
# данные по образцам тканей четырёх типов саркомы
names(Khan)
dim(Khan$xtrain)     # обучающая выборка, предикторы
dim(Khan$xtest)      # тестовая выборка, предикторы
length(Khan$ytrain)  # обучающая выборка, отклик
length(Khan$ytest)   # тестовая выборка, отклик
table(Khan$ytrain)
table(Khan$ytest)

dat <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))

# SVM с линейным ядром
out <- svm(y ~ ., data = dat, kernel = "linear", cost = 10)
summary(out)

# матрица неточностей
table(out$fitted, dat$y)

# тестовые данные
dat.te <- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))

# прогноз на тестовой выборке
pred.te <- predict(out, newdata = dat.te)

# матрица неточностей
table(pred.te, dat.te$y)

```


## Упражнение 9  

Необходимо построить модель на основе SVM для указанной в варианте зависимой переменной.    

Для модели:   

**1** Нарисовать график разброса наблюдений в пространств предикторов, класс показать цветом.   
**2** Указать настроечные параметры метода из своего варианта.   
**3** Подобрать оптимальное значение настроечных параметров по минимальной MSE с перекрёстной проверкой (функция tune).    
**4** Подогнать лучшую модель на обучающей выборке (50% наблюдений).    
**5** Сделать прогноз по лучшей модели на тестовой выборке, оценить его качество точность по матрице неточностей и построить ROC-кривую.   
**6** Сделать выводы о результатах прогноза. Чем объясняется высокая/низкая точность модели?    

В таблице ниже указаны набор данных, столбцы с переменными для модели и метод подгонки.    

**Как сдавать:** прислать на почту преподавателя ссылки:
* на html-отчёт с видимыми блоками кода (блоки кода с параметром echo = T), размещённый на [rpubs.com](rpubs.com).   
* на код, генерирующий отчёт, в репозитории на [github.com](github.com).
В текст отчёта включить постановку задачи и ответы на вопросы задания.  

## Варианты

<table border="1">
<tr>
<td><b>Номер варианта</b></td>
<td><b>Данные</b></td>
<td><b>Зависимая переменная</b></td>
<td><b>Объясняющие переменные</b></td>
<td><b>Метод подгонки моделей</b></td>
</tr>

<tr>
<td>1</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $horsepower$</td>
<td>Классификатор на опорных векторах</td>
</tr>

<tr>
<td>2</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $acceleration$</td>
<td>Классификатор на опорных векторах</td>
</tr>

<tr>
<td>3</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $horsepower$</td>
<td>Машина опорных векторов с полиномиальным ядром второй степени</td>
</tr>

<tr>
<td>4</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $acceleration$</td>
<td>Машина опорных векторов с полиномиальным ядром второй степени</td>
</tr>

<tr>
<td>5</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $horsepower$</td>
<td>Машина опорных векторов с полиномиальным ядром третьей степени</td>
</tr>

<tr>
<td>6</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $acceleration$</td>
<td>Машина опорных векторов с полиномиальным ядром третьей степени</td>
</tr>

<tr>
<td>7</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $horsepower$</td>
<td>Машина опорных векторов с радиальным ядром</td>
</tr>

<tr>
<td>8</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $acceleration$</td>
<td>Машина опорных векторов с радиальным ядром</td>
</tr>

<tr>
<td>9</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$weight$, $horsepower$</td>
<td>Классификатор на опорных векторах</td>
</tr>

<tr>
<td>10</td>
<td>`Auto {ISLR}`</td>
<td>$high.mpg$</td>
<td>$displacement$, $weight$</td>
<td>Машина опорных векторов с полиномиальным ядром второй степени</td>
</tr>

</table>

Переменная high.mpg – высокое значение mpg (сколько автомобиль проходит на галлоне топлива):
$$
high.mpg = \begin{cases} Yes, & если & mpg \ge 23, \\ No, & если & mpg < 23. \end{cases}
$$

*Источники*   

1. *Джеймс Г., Уиттон Д., Хасти Т., Тибширани Р.* Введение в статистическое обучение с примерами на языке R / пер. с англ. С.Э. Мастицкого. -- М.: ДМК Пресс, **2016** -- 450 с. Репозиторий с примерами к книге на русском языке: [https://github.com/ranalytics/islr-ru](https://github.com/ranalytics/islr-ru)    
