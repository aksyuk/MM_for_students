# Esly russkie bukvy ne otobrajautsa: File -> Reopen with encoding... UTF-8

# Используйте UTF-8 как кодировку по умолчанию!
# Установить кодировку в RStudio: Tools -> Global Options -> General, 
#  Default text encoding: UTF-8

# ..............................................................................
# Математическое моделирование: Практика 1
#   Оценка точности модели с непрерывной зависимой переменной (Y)
#      * как делить данные на выборки (обучающую и тестовую)
#      * как считать MSE: среднеквадратическую ошибку модели
#      * как меняются MSE на тестовой и обучающей выборках
#        с изменением гибкости (числа степеней свободы) модели
# ..............................................................................


# Точность модели, непрерывный Y -----------------------------------------------

# Пример из лекции =============================================================

#  Генерируем данные ###########################################################

# ядро
my.seed <- 1486372882

# наблюдений всего
n.all <- 
# доля обучающей выборки
train.percent <- 
# стандартное отклонение случайного шума
res.sd <- 
# границы изменения X
x.min <- 
x.max <- 

# фактические значения x
set.seed(my.seed)
x <- 

# случайный шум
set.seed(my.seed)
res <- 

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- 

# истинная функция взаимосвязи 
y.func <- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3}

# для графика истинной взаимосвязи
x.line <- 
y.line <- 

# фактические значения y (с шумом)
y <- 

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- 
y.train <- 

# наблюдения на тестовой выборке
x.test <- 
y.test <- 

#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot( 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(
      lwd = 2, lty = 2)

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- 
y.model.test <- 

# считаем средний квадрат ошибки на обучающей и тестовой выборке
MSE <- 
names(MSE) <- 
round(MSE, 2)

#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0
head(tbl)

for (i in 2:max.df) {
    # модель
    mod <- 
    
    # модельные значения для расчёта ошибок
    y.model.train <- 
    y.model.test <- 
    
    # считаем средний квадрат ошибки на обучающей и тестовой выборке
    MSE <- 
    
    # записываем результат в таблицу
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)

#  График 2: Зависимость MSE от гибкости модели ################################

plot(
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(
       pch = 21, col = 'red', bg = 'red')

lines( 
      col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(
       lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- 
df.min.MSE.test <- 

# сообщение в консоль
message(paste0('Наименьшая MSE на тестовой выборке равна ', 
               round(min.MSE.test, 2),  
               ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 
my.MSE.test <- 

# сообщение в консоль
message(paste0('Компромисс между точностью и сложностью модели при df = ', 
               df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(
       lty = 2, lwd = 2)
points(
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- 

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(
      lwd = 2, lty = 2)

# модель
lines(
      lwd = 2, col = 'blue')

# легенда
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
